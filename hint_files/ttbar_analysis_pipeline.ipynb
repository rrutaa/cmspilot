{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc37683",
   "metadata": {},
   "source": [
    "# CMS Open Data $t\\bar{t}$: from data delivery to statistical inference\n",
    "\n",
    "We are using [2015 CMS Open Data](https://cms.cern/news/first-cms-open-data-lhc-run-2-released) in this demonstration to showcase an analysis pipeline.\n",
    "It features data delivery and processing, histogram construction and visualization, as well as statistical inference.\n",
    "\n",
    "This notebook was developed in the context of the [IRIS-HEP AGC tools 2022 workshop](https://indico.cern.ch/e/agc-tools-2).\n",
    "This work was supported by the U.S. National Science Foundation (NSF) Cooperative Agreement OAC-1836650 (IRIS-HEP).\n",
    "\n",
    "This is a **technical demonstration**.\n",
    "We are including the relevant workflow aspects that physicists need in their work, but we are not focusing on making every piece of the demonstration physically meaningful.\n",
    "This concerns in particular systematic uncertainties: we capture the workflow, but the actual implementations are more complex in practice.\n",
    "If you are interested in the physics side of analyzing top pair production, check out the latest results from [ATLAS](https://twiki.cern.ch/twiki/bin/view/AtlasPublic/TopPublicResults) and [CMS](https://cms-results.web.cern.ch/cms-results/public-results/preliminary-results/)!\n",
    "If you would like to see more technical demonstrations, also check out an [ATLAS Open Data example](https://indico.cern.ch/event/1076231/contributions/4560405/) demonstrated previously.\n",
    "\n",
    "This notebook implements most of the analysis pipeline shown in the following picture, using the tools also mentioned there:\n",
    "![ecosystem visualization](utils/ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404927fe",
   "metadata": {},
   "source": [
    "### Data pipelines\n",
    "\n",
    "There are two possible pipelines: one with `ServiceX` enabled, and one using only `coffea` for processing.\n",
    "![processing pipelines](utils/processing_pipelines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72d323",
   "metadata": {},
   "source": [
    "### Imports: setting up our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b3b058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "import cabinetry\n",
    "import cloudpickle\n",
    "import correctionlib\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "import copy\n",
    "import hist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyhf\n",
    "\n",
    "import utils  # contains code for bookkeeping and cosmetics, as well as some boilerplate\n",
    "\n",
    "logging.getLogger(\"cabinetry\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd573bb1",
   "metadata": {},
   "source": [
    "### Configuration: number of files and data delivery path\n",
    "\n",
    "The number of files per sample set here determines the size of the dataset we are processing. There are 9 samples being used here, all part of the 2015 CMS Open Data release.\n",
    "\n",
    "These samples were originally published in miniAOD format, but for the purposes of this demonstration were pre-converted into nanoAOD format. More details about the inputs can be found [here](https://github.com/iris-hep/analysis-grand-challenge/tree/main/datasets/cms-open-data-2015).\n",
    "\n",
    "The table below summarizes the amount of data processed depending on the `N_FILES_MAX_PER_SAMPLE` setting.\n",
    "\n",
    "| setting | number of files | total size | number of events |\n",
    "| --- | --- | --- | --- |\n",
    "| `1` | 9 | 22.9 GB | 10,455,719 |\n",
    "| `2` | 18 | 42.8 GB | 19,497,435 |\n",
    "| `5` | 43 | 105 GB | 47,996,231 |\n",
    "| `10` | 79 | 200 GB | 90,546,458 |\n",
    "| `20` | 140 | 359 GB | 163,123,242 |\n",
    "| `50` | 255 | 631 GB | 297,247,463 |\n",
    "| `100` | 395 | 960 GB | 470,397,795 |\n",
    "| `200` | 595 | 1.40 TB | 705,273,291 |\n",
    "| `-1` | 787 | 1.78 TB | 940,160,174 |\n",
    "\n",
    "The input files are all in the 1â€“3 GB range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43e9b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### GLOBAL CONFIGURATION\n",
    "# input files per process, set to e.g. 10 (smaller number = faster)\n",
    "N_FILES_MAX_PER_SAMPLE = 1\n",
    "\n",
    "# enable Dask\n",
    "USE_DASK = True\n",
    "\n",
    "# enable ServiceX, specify options\n",
    "USE_SERVICEX = False\n",
    "USE_SERVICEX_UPROOT_RAW = True # set False to use func_adl instead\n",
    "\n",
    "### ML-INFERENCE SETTINGS\n",
    "\n",
    "# enable ML inference\n",
    "USE_INFERENCE = True\n",
    "\n",
    "# enable inference using NVIDIA Triton server\n",
    "USE_TRITON = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d6520",
   "metadata": {},
   "source": [
    "### Defining our `coffea` Processor\n",
    "\n",
    "The processor includes a lot of the physics analysis details:\n",
    "- event filtering and the calculation of observables,\n",
    "- event weighting,\n",
    "- calculating systematic uncertainties at the event and object level,\n",
    "- filling all the information into histograms that get aggregated and ultimately returned to us by `coffea`.\n",
    "\n",
    "#### Machine Learning Task\n",
    "\n",
    "During the processing step, machine learning is used to calculate one of the variables used for this analysis. The models used are trained separately in the `jetassignment_training.ipynb` notebook. Jets in the events are assigned to labels corresponding with their parent partons using a boosted decision tree (BDT). More information about the model and training can be found within that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee2b3a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TtbarAnalysis(processor.ProcessorABC):\n",
    "    def __init__(self, use_inference, use_triton):\n",
    "\n",
    "        # initialize dictionary of hists for signal and control region\n",
    "        self.hist_dict = {}\n",
    "        for region in [\"4j1b\", \"4j2b\"]:\n",
    "            self.hist_dict[region] = (\n",
    "                hist.Hist.new.Reg(utils.config[\"global\"][\"NUM_BINS\"], \n",
    "                                  utils.config[\"global\"][\"BIN_LOW\"], \n",
    "                                  utils.config[\"global\"][\"BIN_HIGH\"], \n",
    "                                  name=\"observable\", \n",
    "                                  label=\"observable [GeV]\")\n",
    "                .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "                .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "                .Weight()\n",
    "            )\n",
    "        \n",
    "        self.cset = correctionlib.CorrectionSet.from_file(\"corrections.json\")\n",
    "        self.use_inference = use_inference\n",
    "        \n",
    "        # set up attributes only needed if USE_INFERENCE=True\n",
    "        if self.use_inference:\n",
    "            \n",
    "            # initialize dictionary of hists for ML observables\n",
    "            self.ml_hist_dict = {}\n",
    "            for i in range(len(utils.config[\"ml\"][\"FEATURE_NAMES\"])):\n",
    "                self.ml_hist_dict[utils.config[\"ml\"][\"FEATURE_NAMES\"][i]] = (\n",
    "                    hist.Hist.new.Reg(utils.config[\"global\"][\"NUM_BINS\"],\n",
    "                                      utils.config[\"ml\"][\"BIN_LOW\"][i],\n",
    "                                      utils.config[\"ml\"][\"BIN_HIGH\"][i],\n",
    "                                      name=\"observable\",\n",
    "                                      label=utils.config[\"ml\"][\"FEATURE_DESCRIPTIONS\"][i])\n",
    "                    .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "                    .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "                    .Weight()\n",
    "                )\n",
    "            \n",
    "            self.use_triton = use_triton\n",
    "\n",
    "    def only_do_IO(self, events):\n",
    "        for branch in utils.config[\"benchmarking\"][\"IO_BRANCHES\"][\n",
    "            utils.config[\"benchmarking\"][\"IO_FILE_PERCENT\"]\n",
    "        ]:\n",
    "            if \"_\" in branch:\n",
    "                split = branch.split(\"_\")\n",
    "                object_type = split[0]\n",
    "                property_name = \"_\".join(split[1:])\n",
    "                ak.materialized(events[object_type][property_name])\n",
    "            else:\n",
    "                ak.materialized(events[branch])\n",
    "        return {\"hist\": {}}\n",
    "\n",
    "    def process(self, events):\n",
    "        if utils.config[\"benchmarking\"][\"DISABLE_PROCESSING\"]:\n",
    "            # IO testing with no subsequent processing\n",
    "            return self.only_do_IO(events)\n",
    "\n",
    "        # create copies of histogram objects\n",
    "        hist_dict = copy.deepcopy(self.hist_dict)\n",
    "        if self.use_inference:\n",
    "            ml_hist_dict = copy.deepcopy(self.ml_hist_dict)\n",
    "\n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "        variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "\n",
    "        # normalization for MC\n",
    "        x_sec = events.metadata[\"xsec\"]\n",
    "        nevts_total = events.metadata[\"nevts\"]\n",
    "        lumi = 3378 # /pb\n",
    "        if process != \"data\":\n",
    "            xsec_weight = x_sec * lumi / nevts_total\n",
    "        else:\n",
    "            xsec_weight = 1\n",
    "\n",
    "        # setup triton gRPC client or xgboost\n",
    "        if self.use_inference:\n",
    "            if self.use_triton:\n",
    "                triton_client = utils.clients.get_triton_client(utils.config[\"ml\"][\"TRITON_URL\"])\n",
    "            else:\n",
    "                if utils.ml.model_even is None:\n",
    "                    utils.ml.load_models()\n",
    "\n",
    "\n",
    "        #### systematics\n",
    "        # jet energy scale / resolution systematics\n",
    "        # need to adjust schema to instead use coffea add_systematic feature, especially for ServiceX\n",
    "        # cannot attach pT variations to events.jet, so attach to events directly\n",
    "        # and subsequently scale pT by these scale factors\n",
    "        events[\"pt_scale_up\"] = 1.03\n",
    "        events[\"pt_res_up\"] = utils.systematics.jet_pt_resolution(events.Jet.pt)\n",
    "\n",
    "        syst_variations = [\"nominal\"]\n",
    "        jet_kinematic_systs = [\"pt_scale_up\", \"pt_res_up\"]\n",
    "        event_systs = [f\"btag_var_{i}\" for i in range(4)]\n",
    "        if process == \"wjets\":\n",
    "            event_systs.append(\"scale_var\")\n",
    "\n",
    "        # Only do systematics for nominal samples, e.g. ttbar__nominal\n",
    "        if variation == \"nominal\":\n",
    "            syst_variations.extend(jet_kinematic_systs)\n",
    "            syst_variations.extend(event_systs)\n",
    "\n",
    "        # for pt_var in pt_variations:\n",
    "        for syst_var in syst_variations:\n",
    "            ### event selection\n",
    "            # very very loosely based on https://arxiv.org/abs/2006.13076\n",
    "\n",
    "            # Note: This creates new objects, distinct from those in the 'events' object\n",
    "            elecs = events.Electron\n",
    "            muons = events.Muon\n",
    "            jets = events.Jet\n",
    "            if syst_var in jet_kinematic_systs:\n",
    "                # Replace jet.pt with the adjusted values\n",
    "                jets[\"pt\"] = jets.pt * events[syst_var]\n",
    "\n",
    "            electron_reqs = (elecs.pt > 30) & (np.abs(elecs.eta) < 2.1) & (elecs.cutBased == 4) & (elecs.sip3d < 4)\n",
    "            muon_reqs = ((muons.pt > 30) & (np.abs(muons.eta) < 2.1) & (muons.tightId) & (muons.sip3d < 4) &\n",
    "                         (muons.pfRelIso04_all < 0.15))\n",
    "            jet_reqs = (jets.pt > 30) & (np.abs(jets.eta) < 2.4) & (jets.isTightLeptonVeto)\n",
    "\n",
    "            # Only keep objects that pass our requirements\n",
    "            elecs = elecs[electron_reqs]\n",
    "            muons = muons[muon_reqs]\n",
    "            jets = jets[jet_reqs]\n",
    "\n",
    "            if self.use_inference:\n",
    "                even = (events.event%2==0)  # whether events are even/odd\n",
    "\n",
    "            B_TAG_THRESHOLD = 0.5\n",
    "\n",
    "            ######### Store boolean masks with PackedSelection ##########\n",
    "            selections = PackedSelection(dtype='uint64')\n",
    "            # Basic selection criteria\n",
    "            selections.add(\"exactly_1l\", (ak.num(elecs) + ak.num(muons)) == 1)\n",
    "            selections.add(\"atleast_4j\", ak.num(jets) >= 4)\n",
    "            selections.add(\"exactly_1b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) == 1)\n",
    "            selections.add(\"atleast_2b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "            # Complex selection criteria\n",
    "            selections.add(\"4j1b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"exactly_1b\"))\n",
    "            selections.add(\"4j2b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"atleast_2b\"))\n",
    "\n",
    "            for region in [\"4j1b\", \"4j2b\"]:\n",
    "                region_selection = selections.all(region)\n",
    "                region_jets = jets[region_selection]\n",
    "                region_elecs = elecs[region_selection]\n",
    "                region_muons = muons[region_selection]\n",
    "                region_weights = np.ones(len(region_jets)) * xsec_weight\n",
    "                if self.use_inference:\n",
    "                    region_even = even[region_selection]\n",
    "\n",
    "                if region == \"4j1b\":\n",
    "                    observable = ak.sum(region_jets.pt, axis=-1)\n",
    "\n",
    "                elif region == \"4j2b\":\n",
    "\n",
    "                    # reconstruct hadronic top as bjj system with largest pT\n",
    "                    trijet = ak.combinations(region_jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "                    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # calculate four-momentum of tri-jet system\n",
    "                    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "                    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "                    # pick trijet candidate with largest pT and calculate mass of system\n",
    "                    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "                    observable = ak.flatten(trijet_mass)\n",
    "\n",
    "                    if sum(region_selection)==0:\n",
    "                        continue\n",
    "\n",
    "                    if self.use_inference:\n",
    "                        features, perm_counts = utils.ml.get_features(\n",
    "                            region_jets,\n",
    "                            region_elecs,\n",
    "                            region_muons,\n",
    "                            max_n_jets=utils.config[\"ml\"][\"MAX_N_JETS\"],\n",
    "                        )\n",
    "                        even_perm = np.repeat(region_even, perm_counts)\n",
    "\n",
    "                        # calculate ml observable\n",
    "                        if self.use_triton:\n",
    "                            results = utils.ml.get_inference_results_triton(\n",
    "                                features,\n",
    "                                even_perm,\n",
    "                                triton_client,\n",
    "                                utils.config[\"ml\"][\"MODEL_NAME\"],\n",
    "                                utils.config[\"ml\"][\"MODEL_VERSION_EVEN\"],\n",
    "                                utils.config[\"ml\"][\"MODEL_VERSION_ODD\"],\n",
    "                            )\n",
    "\n",
    "                        else:\n",
    "                            results = utils.ml.get_inference_results_local(\n",
    "                                features,\n",
    "                                even_perm,\n",
    "                                utils.ml.model_even,\n",
    "                                utils.ml.model_odd,\n",
    "                            )\n",
    "                            \n",
    "                        results = ak.unflatten(results, perm_counts)\n",
    "                        features = ak.flatten(ak.unflatten(features, perm_counts)[\n",
    "                            ak.from_regular(ak.argmax(results,axis=1)[:, np.newaxis])\n",
    "                        ])\n",
    "                syst_var_name = f\"{syst_var}\"\n",
    "                # Break up the filling into event weight systematics and object variation systematics\n",
    "                if syst_var in event_systs:\n",
    "                    for i_dir, direction in enumerate([\"up\", \"down\"]):\n",
    "                        # Should be an event weight systematic with an up/down variation\n",
    "                        if syst_var.startswith(\"btag_var\"):\n",
    "                            i_jet = int(syst_var.rsplit(\"_\",1)[-1])   # Kind of fragile\n",
    "                            wgt_variation = self.cset[\"event_systematics\"].evaluate(\"btag_var\", direction, region_jets.pt[:,i_jet])\n",
    "                        elif syst_var == \"scale_var\":\n",
    "                            # The pt array is only used to make sure the output array has the correct shape\n",
    "                            wgt_variation = self.cset[\"event_systematics\"].evaluate(\"scale_var\", direction, region_jets.pt[:,0])\n",
    "                        syst_var_name = f\"{syst_var}_{direction}\"\n",
    "                        hist_dict[region].fill(\n",
    "                            observable=observable, process=process,\n",
    "                            variation=syst_var_name, weight=region_weights * wgt_variation\n",
    "                        )\n",
    "                        if region == \"4j2b\" and self.use_inference:\n",
    "                            for i in range(len(utils.config[\"ml\"][\"FEATURE_NAMES\"])):\n",
    "                                ml_hist_dict[utils.config[\"ml\"][\"FEATURE_NAMES\"][i]].fill(\n",
    "                                    observable=features[..., i], process=process,\n",
    "                                    variation=syst_var_name, weight=region_weights * wgt_variation\n",
    "                                )\n",
    "                else:\n",
    "                    # Should either be 'nominal' or an object variation systematic\n",
    "                    if variation != \"nominal\":\n",
    "                        # This is a 2-point systematic, e.g. ttbar__scaledown, ttbar__ME_var, etc.\n",
    "                        syst_var_name = variation\n",
    "                    hist_dict[region].fill(\n",
    "                        observable=observable, process=process,\n",
    "                        variation=syst_var_name, weight=region_weights\n",
    "                    )\n",
    "                    if region == \"4j2b\" and self.use_inference:\n",
    "                        for i in range(len(utils.config[\"ml\"][\"FEATURE_NAMES\"])):\n",
    "                            ml_hist_dict[utils.config[\"ml\"][\"FEATURE_NAMES\"][i]].fill(\n",
    "                                observable=features[..., i], process=process,\n",
    "                                variation=syst_var_name, weight=region_weights\n",
    "                            )\n",
    "\n",
    "\n",
    "        output = {\"nevents\": {events.metadata[\"dataset\"]: len(events)}, \"hist_dict\": hist_dict}\n",
    "        if self.use_inference:\n",
    "            output[\"ml_hist_dict\"] = ml_hist_dict\n",
    "\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd4c9e",
   "metadata": {},
   "source": [
    "### \"Fileset\" construction and metadata\n",
    "\n",
    "Here, we gather all the required information about the files we want to process: paths to the files and asociated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf166ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processes in fileset: ['ttbar__nominal', 'ttbar__scaledown', 'ttbar__scaleup', 'ttbar__ME_var', 'ttbar__PS_var', 'single_top_s_chan__nominal', 'single_top_t_chan__nominal', 'single_top_tW__nominal', 'wjets__nominal']\n",
      "\n",
      "example of information in fileset:\n",
      "{\n",
      "  'files': [https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_00000_0000.root, ...],\n",
      "  'metadata': {'process': 'ttbar', 'variation': 'nominal', 'nevts': 6389801, 'xsec': 729.84}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fileset = utils.file_input.construct_fileset(\n",
    "    N_FILES_MAX_PER_SAMPLE,\n",
    "    use_xcache=False,\n",
    "    af_name=utils.config[\"benchmarking\"][\"AF_NAME\"],  # local files on /data for af_name=\"ssl-dev\"\n",
    "    input_from_eos=utils.config[\"benchmarking\"][\"INPUT_FROM_EOS\"],\n",
    "    xcache_atlas_prefix=utils.config[\"benchmarking\"][\"XCACHE_ATLAS_PREFIX\"],\n",
    ")\n",
    "\n",
    "print(f\"processes in fileset: {list(fileset.keys())}\")\n",
    "print(f\"\\nexample of information in fileset:\\n{{\\n  'files': [{fileset['ttbar__nominal']['files'][0]}, ...],\")\n",
    "print(f\"  'metadata': {fileset['ttbar__nominal']['metadata']}\\n}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910d3d5",
   "metadata": {},
   "source": [
    "### ServiceX-specific functionality: query setup\n",
    "\n",
    "Use one of two query languages (func_adl and uproot-raw) to define the query to be used for the purpose of extracting columns and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a032a148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_query(source):\n",
    "    \"\"\"Query for event / column selection: >=4j >=1b, ==1 lep with pT>30 GeV + additional cuts,\n",
    "    return relevant columns\n",
    "    *NOTE* jet pT cut is set lower to account for systematic variations to jet pT\n",
    "    \"\"\"\n",
    "    cuts = source.FromTree(\"Events\")\\\n",
    "                 .Where(lambda e: {\"pt\": e.Electron_pt,\n",
    "                               \"eta\": e.Electron_eta,\n",
    "                               \"cutBased\": e.Electron_cutBased,\n",
    "                               \"sip3d\": e.Electron_sip3d,}.Zip()\\\n",
    "                        .Where(lambda electron: (electron.pt > 30\n",
    "                                                 and abs(electron.eta) < 2.1\n",
    "                                                 and electron.cutBased == 4\n",
    "                                                 and electron.sip3d < 4)).Count()\n",
    "                        + {\"pt\": e.Muon_pt,\n",
    "                           \"eta\": e.Muon_eta,\n",
    "                           \"tightId\": e.Muon_tightId,\n",
    "                           \"sip3d\": e.Muon_sip3d,\n",
    "                           \"pfRelIso04_all\": e.Muon_pfRelIso04_all}.Zip()\\\n",
    "                        .Where(lambda muon: (muon.pt > 30\n",
    "                                             and abs(muon.eta) < 2.1\n",
    "                                             and muon.tightId\n",
    "                                             and muon.pfRelIso04_all < 0.15)).Count()== 1)\\\n",
    "                        .Where(lambda f: {\"pt\": f.Jet_pt,\n",
    "                                          \"eta\": f.Jet_eta,\n",
    "                                          \"jetId\": f.Jet_jetId}.Zip()\\\n",
    "                               .Where(lambda jet: (jet.pt > 25\n",
    "                                                   and abs(jet.eta) < 2.4\n",
    "                                                   and jet.jetId == 6)).Count() >= 4)\\\n",
    "                        .Where(lambda g: {\"pt\": g.Jet_pt,\n",
    "                                          \"eta\": g.Jet_eta,\n",
    "                                          \"btagCSVV2\": g.Jet_btagCSVV2,\n",
    "                                          \"jetId\": g.Jet_jetId}.Zip()\\\n",
    "                        .Where(lambda jet: (jet.btagCSVV2 > 0.5\n",
    "                                            and jet.pt > 25\n",
    "                                            and abs(jet.eta) < 2.4)\n",
    "                                            and jet.jetId == 6).Count() >= 1)\n",
    "    selection = cuts.Select(lambda h: {\"Electron_pt\": h.Electron_pt,\n",
    "                                       \"Electron_eta\": h.Electron_eta,\n",
    "                                       \"Electron_phi\": h.Electron_phi,\n",
    "                                       \"Electron_mass\": h.Electron_mass,\n",
    "                                       \"Electron_cutBased\": h.Electron_cutBased,\n",
    "                                       \"Electron_sip3d\": h.Electron_sip3d,\n",
    "                                       \"Muon_pt\": h.Muon_pt,\n",
    "                                       \"Muon_eta\": h.Muon_eta,\n",
    "                                       \"Muon_phi\": h.Muon_phi,\n",
    "                                       \"Muon_mass\": h.Muon_mass,\n",
    "                                       \"Muon_tightId\": h.Muon_tightId,\n",
    "                                       \"Muon_sip3d\": h.Muon_sip3d,\n",
    "                                       \"Muon_pfRelIso04_all\": h.Muon_pfRelIso04_all,\n",
    "                                       \"Jet_mass\": h.Jet_mass,\n",
    "                                       \"Jet_pt\": h.Jet_pt,\n",
    "                                       \"Jet_eta\": h.Jet_eta,\n",
    "                                       \"Jet_phi\": h.Jet_phi,\n",
    "                                       \"Jet_qgl\": h.Jet_qgl,\n",
    "                                       \"Jet_btagCSVV2\": h.Jet_btagCSVV2,\n",
    "                                       \"Jet_jetId\": h.Jet_jetId,\n",
    "                                       \"event\": h.event,\n",
    "                                      })\n",
    "    if USE_INFERENCE:\n",
    "        return selection\n",
    "\n",
    "    # some branches are only needed if USE_INFERENCE is turned on\n",
    "    return selection.Select(lambda h: {\"Electron_pt\": h.Electron_pt,\n",
    "                                       \"Electron_eta\": h.Electron_eta,\n",
    "                                       \"Electron_cutBased\": h.Electron_cutBased,\n",
    "                                       \"Electron_sip3d\": h.Electron_sip3d,\n",
    "                                       \"Muon_pt\": h.Muon_pt,\n",
    "                                       \"Muon_eta\": h.Muon_eta,\n",
    "                                       \"Muon_tightId\": h.Muon_tightId,\n",
    "                                       \"Muon_sip3d\": h.Muon_sip3d,\n",
    "                                       \"Muon_pfRelIso04_all\": h.Muon_pfRelIso04_all,\n",
    "                                       \"Jet_mass\": h.Jet_mass,\n",
    "                                       \"Jet_pt\": h.Jet_pt,\n",
    "                                       \"Jet_eta\": h.Jet_eta,\n",
    "                                       \"Jet_phi\": h.Jet_phi,\n",
    "                                       \"Jet_btagCSVV2\": h.Jet_btagCSVV2,\n",
    "                                       \"Jet_jetId\": h.Jet_jetId,\n",
    "                                      })\n",
    "\n",
    "def get_uproot_raw_query():\n",
    "    cut = '((count_nonzero((Electron_pt > 30) & (abs(Electron_eta) < 2.1) & (Electron_cutBased == 4) & (Electron_sip3d < 4), axis=1)' \\\n",
    "          '+ count_nonzero((Muon_pt > 30) & (abs(Muon_eta) < 2.1) & (Muon_tightId) & (Muon_pfRelIso04_all < 0.15), axis=1)) == 1)' \\\n",
    "          '& (count_nonzero((Jet_pt > 25) & (abs(Jet_eta) < 2.4) & (Jet_jetId == 6), axis=1) >= 4)' \\\n",
    "          '& (count_nonzero((Jet_pt > 25) & (abs(Jet_eta) < 2.4) & (Jet_jetId == 6) & (Jet_btagCSVV2 > 0.5), axis=1) >= 1)'\n",
    "    branch_filter =  ['Electron_pt',\n",
    "                      'Electron_eta',\n",
    "                      'Electron_cutBased',\n",
    "                      'Electron_sip3d',\n",
    "                      'Muon_pt',\n",
    "                      'Muon_eta',\n",
    "                      'Muon_tightId',\n",
    "                      'Muon_sip3d',\n",
    "                      'Muon_pfRelIso04_all',\n",
    "                      'Jet_mass',\n",
    "                      'Jet_pt',\n",
    "                      'Jet_eta',\n",
    "                      'Jet_phi',\n",
    "                      'Jet_qgl',\n",
    "                      'Jet_btagCSVV2',\n",
    "                      'Jet_jetId',\n",
    "                     ]\n",
    "    if USE_INFERENCE:\n",
    "        branch_filter += [\n",
    "                      'Electron_phi',\n",
    "                      'Electron_mass',\n",
    "                      'Muon_phi',\n",
    "                      'Muon_mass',\n",
    "                      'event',\n",
    "        ]\n",
    "    return query.UprootRaw({'treename': {'Events': 'servicex'}, 'cut': cut, 'filter_name': branch_filter})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f08fc1",
   "metadata": {},
   "source": [
    "### Caching the queried datasets with `ServiceX`\n",
    "\n",
    "Using the queries created with `func_adl` or `uproot-raw`, we are using `ServiceX` to read the CMS Open Data files to build cached files with only the specific event information as dictated by the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d4f4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_SERVICEX:\n",
    "    from servicex import deliver, query, dataset\n",
    "    # dummy dataset on which to generate the query\n",
    "    if USE_SERVICEX_UPROOT_RAW:\n",
    "        hl_query = get_uproot_raw_query()\n",
    "    else:\n",
    "        hl_query = get_query(query.FuncADL_Uproot())\n",
    "\n",
    "    # now we query the files using a wrapper around ServiceXDataset to transform all processes at once\n",
    "    t0 = time.time()\n",
    "\n",
    "    bundle = { 'Sample': [ { 'Name': _[0], 'Dataset': dataset.FileList(_[1]['files']),\n",
    "                            'Query': hl_query,\n",
    "                            'IgnoreLocalCache': utils.config[\"global\"][\"SERVICEX_IGNORE_CACHE\"]\n",
    "                           }\n",
    "                           for _ in fileset.items() ] }\n",
    "    if not utils.config[\"global\"][\"USE_SERVICEX_DOWNLOAD\"]:\n",
    "        bundle['General'] = { 'Delivery': 'URLs' }\n",
    "    files_per_process = deliver(bundle)\n",
    "\n",
    "    print(f\"ServiceX data delivery took {time.time() - t0:.2f} seconds\")\n",
    "\n",
    "    # update fileset to point to ServiceX-transformed files\n",
    "    for process in files_per_process.keys():\n",
    "        fileset[process][\"files\"] = files_per_process[process]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a9e49",
   "metadata": {},
   "source": [
    "### Execute the data delivery pipeline\n",
    "\n",
    "What happens here depends on the flag `USE_SERVICEX`. If set to true, the processor is run on the data previously gathered by ServiceX, then will gather output histograms.\n",
    "\n",
    "When `USE_SERVICEX` is false, the input files need to be processed during this step as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c59d30d2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Timed out trying to connect to tls://localhost:8786 after 30 s",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mCommClosedError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/distributed/comm/core.py:342\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(addr, timeout, deserialize, handshake_overrides, **connection_args)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 342\u001b[0m     comm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m wait_for(\n\u001b[1;32m    343\u001b[0m         connector\u001b[38;5;241m.\u001b[39mconnect(loc, deserialize\u001b[38;5;241m=\u001b[39mdeserialize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconnection_args),\n\u001b[1;32m    344\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(intermediate_cap, time_left()),\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/distributed/utils.py:1915\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwait_for\u001b[39m(fut: Awaitable[T], timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m-> 1915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(fut, timeout)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/asyncio/tasks.py:445\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py:560\u001b[0m, in \u001b[0;36mBaseTCPConnector.connect\u001b[0;34m(self, address, deserialize, **connection_args)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m StreamClosedError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;66;03m# The socket connect() call failed\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m     \u001b[43mconvert_stream_closed_error\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLCertVerificationError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py:143\u001b[0m, in \u001b[0;36mconvert_stream_closed_error\u001b[0;34m(obj, exc)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FatalCommClosedError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m CommClosedError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mCommClosedError\u001b[0m: in <distributed.comm.tcp.TLSConnector object at 0x7f8ed405feb0>: ConnectionRefusedError: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m USE_DASK:\n\u001b[1;32m      3\u001b[0m     cloudpickle\u001b[38;5;241m.\u001b[39mregister_pickle_by_value(utils) \u001b[38;5;66;03m# serialize methods and objects in utils so that they can be accessed within the coffea processor\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     executor \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mDaskExecutor(client\u001b[38;5;241m=\u001b[39m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43maf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglobal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     executor \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mFuturesExecutor(workers\u001b[38;5;241m=\u001b[39mutils\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenchmarking\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_CORES\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/analysis-grand-challenge/analyses/cms-open-data-ttbar/utils/clients.py:5\u001b[0m, in \u001b[0;36mget_client\u001b[0;34m(af)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m af \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoffea_casa\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[0;32m----> 5\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtls://localhost:8786\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m af \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEAF\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhtcdaskgateway\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTCGateway\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/distributed/client.py:1230\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, address, loop, timeout, set_as_default, scheduler_file, security, asynchronous, name, heartbeat_interval, serializers, deserializers, extensions, direct_to_workers, connection_limit, **kwargs)\u001b[0m\n\u001b[1;32m   1227\u001b[0m preload_argv \u001b[38;5;241m=\u001b[39m dask\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistributed.client.preload-argv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreloads \u001b[38;5;241m=\u001b[39m preloading\u001b[38;5;241m.\u001b[39mprocess_preloads(\u001b[38;5;28mself\u001b[39m, preload, preload_argv)\n\u001b[0;32m-> 1230\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1231\u001b[0m Client\u001b[38;5;241m.\u001b[39m_instances\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecreate_tasks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReplayTaskClient\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/distributed/client.py:1432\u001b[0m, in \u001b[0;36mClient.start\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_started \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1432\u001b[0m     \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/distributed/utils.py:439\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m         wait(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/distributed/utils.py:413\u001b[0m, in \u001b[0;36msync.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m    411\u001b[0m         awaitable \u001b[38;5;241m=\u001b[39m wait_for(awaitable, timeout)\n\u001b[1;32m    412\u001b[0m     future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(awaitable)\n\u001b[0;32m--> 413\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m future\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m    415\u001b[0m     error \u001b[38;5;241m=\u001b[39m exception\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tornado/gen.py:766\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 766\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    768\u001b[0m         \u001b[38;5;66;03m# Save the exception for later. It's important that\u001b[39;00m\n\u001b[1;32m    769\u001b[0m         \u001b[38;5;66;03m# gen.throw() not be called inside this try/except block\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;66;03m# because that makes sys.exc_info behave unexpectedly.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m         exc: Optional[\u001b[38;5;167;01mException\u001b[39;00m] \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/distributed/client.py:1511\u001b[0m, in \u001b[0;36mClient._start\u001b[0;34m(self, timeout, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_comm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_connected(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/distributed/client.py:1579\u001b[0m, in \u001b[0;36mClient._ensure_connected\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connecting_to_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1579\u001b[0m     comm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m connect(\n\u001b[1;32m   1580\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39maddress, timeout\u001b[38;5;241m=\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_args\n\u001b[1;32m   1581\u001b[0m     )\n\u001b[1;32m   1582\u001b[0m     comm\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClient->Scheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/distributed/comm/core.py:368\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(addr, timeout, deserialize, handshake_overrides, **connection_args)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(backoff)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimed out trying to connect to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maddr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    370\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mactive_exception\u001b[39;00m\n\u001b[1;32m    372\u001b[0m local_info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomm\u001b[38;5;241m.\u001b[39mhandshake_info(),\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(handshake_overrides \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[1;32m    375\u001b[0m }\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mwrite(local_info)\n",
      "\u001b[0;31mOSError\u001b[0m: Timed out trying to connect to tls://localhost:8786 after 30 s"
     ]
    }
   ],
   "source": [
    "NanoAODSchema.warn_missing_crossrefs = False # silences warnings about branches we will not use here\n",
    "if USE_DASK:\n",
    "    cloudpickle.register_pickle_by_value(utils) # serialize methods and objects in utils so that they can be accessed within the coffea processor\n",
    "    executor = processor.DaskExecutor(client=utils.clients.get_client(af=utils.config[\"global\"][\"AF\"]))\n",
    "else:\n",
    "    executor = processor.FuturesExecutor(workers=utils.config[\"benchmarking\"][\"NUM_CORES\"])\n",
    "\n",
    "run = processor.Runner(\n",
    "    executor=executor,\n",
    "    schema=NanoAODSchema,\n",
    "    savemetrics=True,\n",
    "    metadata_cache={},\n",
    "    chunksize=utils.config[\"benchmarking\"][\"CHUNKSIZE\"])\n",
    "\n",
    "if USE_SERVICEX:\n",
    "    treename = \"servicex\"\n",
    "else:\n",
    "    treename = \"Events\"\n",
    "\n",
    "# load local models if not using Triton or FuturesExecutor and models are not yet loaded\n",
    "if USE_INFERENCE and not USE_TRITON and USE_DASK and utils.ml.model_even is None and utils.ml.model_odd is None:\n",
    "    utils.ml.load_models()\n",
    "\n",
    "filemeta = run.preprocess(fileset, treename=treename)  # pre-processing\n",
    "\n",
    "t0 = time.monotonic()\n",
    "# processing\n",
    "all_histograms, metrics = run(\n",
    "    fileset,\n",
    "    treename,\n",
    "    processor_instance=TtbarAnalysis(USE_INFERENCE, USE_TRITON)\n",
    ")\n",
    "exec_time = time.monotonic() - t0\n",
    "\n",
    "print(f\"\\nexecution took {exec_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f0dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track metrics\n",
    "utils.metrics.track_metrics(metrics, fileset, exec_time, USE_DASK, USE_SERVICEX, N_FILES_MAX_PER_SAMPLE, USE_INFERENCE, USE_TRITON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb4428",
   "metadata": {},
   "source": [
    "### Inspecting the produced histograms\n",
    "\n",
    "Let's have a look at the data we obtained.\n",
    "We built histograms in two phase space regions, for multiple physics processes and systematic variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd348fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils.plotting  # noqa: E402\n",
    "\n",
    "utils.plotting.set_style()\n",
    "\n",
    "all_histograms[\"hist_dict\"][\"4j1b\"][120j::hist.rebin(2), :, \"nominal\"].stack(\"process\")[::-1].plot(stack=True, histtype=\"fill\", linewidth=1, edgecolor=\"grey\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\"$\\geq$ 4 jets, 1 b-tag\")\n",
    "plt.xlabel(\"$H_T$ [GeV]\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c902d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_histograms[\"hist_dict\"][\"4j2b\"][:, :, \"nominal\"].stack(\"process\")[::-1].plot(stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\"$\\geq$ 4 jets, $\\geq$ 2 b-tags\")\n",
    "plt.xlabel(\"$m_{bjj}$ [GeV]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3df8b",
   "metadata": {},
   "source": [
    "Our top reconstruction approach ($bjj$ system with largest $p_T$) has worked!\n",
    "\n",
    "Let's also have a look at some systematic variations:\n",
    "- b-tagging, which we implemented as jet-kinematic dependent event weights,\n",
    "- jet energy variations, which vary jet kinematics, resulting in acceptance effects and observable changes.\n",
    "\n",
    "We are making of [UHI](https://uhi.readthedocs.io/) here to re-bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1aabfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# b-tagging variations\n",
    "all_histograms[\"hist_dict\"][\"4j1b\"][120j::hist.rebin(2), \"ttbar\", \"nominal\"].plot(label=\"nominal\", linewidth=2)\n",
    "all_histograms[\"hist_dict\"][\"4j1b\"][120j::hist.rebin(2), \"ttbar\", \"btag_var_0_up\"].plot(label=\"NP 1\", linewidth=2)\n",
    "all_histograms[\"hist_dict\"][\"4j1b\"][120j::hist.rebin(2), \"ttbar\", \"btag_var_1_up\"].plot(label=\"NP 2\", linewidth=2)\n",
    "all_histograms[\"hist_dict\"][\"4j1b\"][120j::hist.rebin(2), \"ttbar\", \"btag_var_2_up\"].plot(label=\"NP 3\", linewidth=2)\n",
    "all_histograms[\"hist_dict\"][\"4j1b\"][120j::hist.rebin(2), \"ttbar\", \"btag_var_3_up\"].plot(label=\"NP 4\", linewidth=2)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"$H_T$ [GeV]\")\n",
    "plt.title(\"b-tagging variations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f89e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# jet energy scale variations\n",
    "all_histograms[\"hist_dict\"][\"4j2b\"][:, \"ttbar\", \"nominal\"].plot(label=\"nominal\", linewidth=2)\n",
    "all_histograms[\"hist_dict\"][\"4j2b\"][:, \"ttbar\", \"pt_scale_up\"].plot(label=\"scale up\", linewidth=2)\n",
    "all_histograms[\"hist_dict\"][\"4j2b\"][:, \"ttbar\", \"pt_res_up\"].plot(label=\"resolution up\", linewidth=2)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"$m_{bjj}$ [Gev]\")\n",
    "plt.title(\"Jet energy variations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa616ac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ML inference variables\n",
    "if USE_INFERENCE:\n",
    "    fig, axs = plt.subplots(10,2,figsize=(14,40))\n",
    "    for i in range(len(utils.config[\"ml\"][\"FEATURE_NAMES\"])):\n",
    "        if i<10: \n",
    "            column=0\n",
    "            row=i\n",
    "        else: \n",
    "            column=1\n",
    "            row=i-10\n",
    "        all_histograms['ml_hist_dict'][utils.config[\"ml\"][\"FEATURE_NAMES\"][i]][:, :, \"nominal\"].stack(\"process\").project(\"observable\").plot(\n",
    "            stack=True, \n",
    "            histtype=\"fill\", \n",
    "            linewidth=1, \n",
    "            edgecolor=\"grey\", \n",
    "            ax=axs[row,column]\n",
    "        )\n",
    "        axs[row, column].legend(frameon=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c334dd3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Save histograms to disk\n",
    "\n",
    "We'll save everything to disk for subsequent usage.\n",
    "This also builds pseudo-data by combining events from the various simulation setups we have processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d05ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.file_output.save_histograms(all_histograms['hist_dict'], \"histograms.root\")\n",
    "\n",
    "if USE_INFERENCE:\n",
    "    utils.file_output.save_histograms(all_histograms['ml_hist_dict'], \"histograms_ml.root\", add_offset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904cd3c",
   "metadata": {},
   "source": [
    "### Statistical inference\n",
    "\n",
    "We are going to perform a re-binning for the statistical inference.\n",
    "This is planned to be conveniently provided via cabinetry (see [cabinetry#412](https://github.com/scikit-hep/cabinetry/issues/412), but in the meantime we can achieve this via [template building overrides](https://cabinetry.readthedocs.io/en/latest/advanced.html#overrides-for-template-building).\n",
    "The implementation is provided in a function in `utils/`.\n",
    "\n",
    "A statistical model has been defined in `config.yml`, ready to be used with our output.\n",
    "We will use `cabinetry` to combine all histograms into a `pyhf` workspace and fit the resulting statistical model to the pseudodata we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b89fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils.rebinning  # noqa: E402\n",
    "\n",
    "cabinetry_config = cabinetry.configuration.load(\"cabinetry_config.yml\")\n",
    "\n",
    "# rebinning: lower edge 110 GeV, merge bins 2->1\n",
    "rebinning_router = utils.rebinning.get_cabinetry_rebinning_router(cabinetry_config, rebinning=slice(110j, None, hist.rebin(2)))\n",
    "cabinetry.templates.build(cabinetry_config, router=rebinning_router)\n",
    "cabinetry.templates.postprocess(cabinetry_config)  # optional post-processing (e.g. smoothing)\n",
    "ws = cabinetry.workspace.build(cabinetry_config)\n",
    "cabinetry.workspace.save(ws, \"workspace.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36dc601",
   "metadata": {},
   "source": [
    "We can inspect the workspace with `pyhf`, or use `pyhf` to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a83712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pyhf inspect workspace.json | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e4361",
   "metadata": {},
   "source": [
    "Let's try out what we built: the next cell will perform a maximum likelihood fit of our statistical model to the pseudodata we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358d17dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, data = cabinetry.model_utils.model_and_data(ws)\n",
    "fit_results = cabinetry.fit.fit(model, data)\n",
    "\n",
    "cabinetry.visualize.pulls(\n",
    "    fit_results, exclude=\"ttbar_norm\", close_figure=True, save_figure=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd480eec",
   "metadata": {},
   "source": [
    "For this pseudodata, what is the resulting ttbar cross-section divided by the Standard Model prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ffdf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "poi_index = model.config.poi_index\n",
    "print(f\"\\nfit result for ttbar_norm: {fit_results.bestfit[poi_index]:.3f} +/- {fit_results.uncertainty[poi_index]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a293479",
   "metadata": {},
   "source": [
    "Let's also visualize the model before and after the fit, in both the regions we are using.\n",
    "The binning here corresponds to the binning used for the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfab0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = cabinetry.model_utils.prediction(model)\n",
    "model_prediction_postfit = cabinetry.model_utils.prediction(model, fit_results=fit_results)\n",
    "figs = cabinetry.visualize.data_mc(model_prediction, data, close_figure=True, config=cabinetry_config)\n",
    "# below method reimplements this visualization in a grid view\n",
    "utils.plotting.plot_data_mc(model_prediction, model_prediction_postfit, data, cabinetry_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc4b23",
   "metadata": {},
   "source": [
    "### ML Validation\n",
    "We can further validate our results by applying the above fit to different ML observables and checking for good agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c7e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the ml workspace (uses the ml observable instead of previous method)\n",
    "if USE_INFERENCE:\n",
    "    config_ml = cabinetry.configuration.load(\"cabinetry_config_ml.yml\")\n",
    "    cabinetry.templates.collect(config_ml)\n",
    "    cabinetry.templates.postprocess(config_ml)  # optional post-processing (e.g. smoothing)\n",
    "\n",
    "    ws_ml = cabinetry.workspace.build(config_ml)\n",
    "    ws_pruned = pyhf.Workspace(ws_ml).prune(channels=[\"Feature3\", \"Feature8\", \"Feature9\",\n",
    "                                                      \"Feature10\", \"Feature11\", \"Feature12\",\n",
    "                                                      \"Feature13\", \"Feature14\", \"Feature15\",\n",
    "                                                      \"Feature16\", \"Feature17\", \"Feature18\",\n",
    "                                                      \"Feature19\"])\n",
    "\n",
    "    cabinetry.workspace.save(ws_pruned, \"workspace_ml.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad987f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_INFERENCE:\n",
    "    model_ml, data_ml = cabinetry.model_utils.model_and_data(ws_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f60c316",
   "metadata": {},
   "source": [
    "We have a channel for each ML observable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e36bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyhf inspect workspace_ml.json | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain model prediction before and after fit\n",
    "if USE_INFERENCE:\n",
    "    model_prediction_ml = cabinetry.model_utils.prediction(model_ml)\n",
    "    fit_results_mod = cabinetry.model_utils.match_fit_results(model_ml, fit_results)\n",
    "    model_prediction_postfit_ml = cabinetry.model_utils.prediction(model_ml, fit_results=fit_results_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee708250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_INFERENCE:\n",
    "    utils.plotting.plot_data_mc(model_prediction_ml, model_prediction_postfit_ml, data_ml, config_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54967894",
   "metadata": {},
   "outputs": [],
   "source": [
    "if utils.config[\"preservation\"][\"HEPData\"] is True:\n",
    "    import utils.hepdata\n",
    "    #Submission of model prediction\n",
    "    utils.hepdata.preparing_hep_data_format(model, model_prediction, \"hepdata_model\", cabinetry_config)\n",
    "    #Submission of model_ml prediction\n",
    "    utils.hepdata.preparing_hep_data_format(model_ml, model_prediction_ml,\"hepdata_model_ml\", config_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce2d14",
   "metadata": {},
   "source": [
    "### What is next?\n",
    "\n",
    "Our next goals for this pipeline demonstration are:\n",
    "- making this analysis even **more feature-complete**,\n",
    "- **addressing performance bottlenecks** revealed by this demonstrator,\n",
    "- **collaborating** with you!\n",
    "\n",
    "Please do not hesitate to get in touch if you would like to join the effort, or are interested in re-implementing (pieces of) the pipeline with different tools!\n",
    "\n",
    "Our mailing list is analysis-grand-challenge@iris-hep.org, sign up via the [Google group](https://groups.google.com/a/iris-hep.org/g/analysis-grand-challenge)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
